{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOEE3250/SOEE5675M/SOEE5116\t\t\t\t\t2024/25\n",
    "\n",
    "Inverse Theory\n",
    "\n",
    "# Practical 3: Best Linear Unbiased Estimator (BLUE)\n",
    "\n",
    "## Geophysical background\n",
    "A building is slowly subsiding (sinking) with time. The height of the building is assumed to follow an exponential decay of the form:\n",
    "\n",
    "$h=h_0+A (e^{-t/150}-1 )$\n",
    "\n",
    "where $h$ is height (in metres), $h_0$ is the height at time $t=0$, $t$ is the time in days and $A$ is a constant representing the total subsidence at infinite time. The height of the building is measured using precise GPS every 60 days for 300 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors of the height measurements have zero mean, are assumed independent, and their standard deviations are:\n",
    "\n",
    "| Time(days)   | Standard deviation (cm)  |\n",
    "| :------ |---------:|\n",
    "|  0  | 3 |\n",
    "|  60 | 5 |\n",
    "| 120 | 2 | \n",
    "| 180 | 1 |\n",
    "| 240 | 3 | \n",
    "| 300 | 2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse theory problem\n",
    "\n",
    "The aim is to create your own dataset then demonstrate that the BLUE is unbiased and gives better results than the unweighted least squares estimator. To achieve this, you will simulate measurements for a given set of model parameters and then use the BLUE (as well as unweighted least squares) to invert the problem. Simulation of data has the advantage that you know what the true values are for the model parameters, which enables you to assess how successful the estimator is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) In markdown, write down the model vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) In Python, define a vector of times, and the matrix G. Display G to the screen using the 'print' command.\n",
    "You can implement the exponential function using np.exp(), and pass it a **vector** of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = np.array([.....])\n",
    "# create a matrix of ones, then overwrite the entries that are not ones\n",
    "#G = np.ones((6,2))\n",
    "#G[:,1]=  #define the multiplier of model parameter A in one line of Python\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Define a vector, m_true, assuming that $h_0=140$ m and $A=0.15$ m. Now calculate what the measurements would be if there were no measurement errors using your forward model. Define this as the vector d_true. \n",
    "Plot d_true against time as blue circles. Label both axes and provide a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_true = np.array([....])\n",
    "#d_true = # G * m_true. In Python, use \"@\" for matrix multiplication\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(x,y 'o',color='blue')\n",
    "#plt.ylabel('')\n",
    "#plt.xlabel('');\n",
    "#plt.title('My title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) Define the variance-covariance matrix $Q_{dd}$ for the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easiest to use the np.diag() function.\n",
    "# define a vector of standard deviations, then use this in an argument to the np.diag function.\n",
    "# sd = np.array([....])\n",
    "# Qdd = np.diag(...)\n",
    "print(Qdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5) Calculate some random noise to represent the error for each of the measurements, assuming the noise is drawn from a Gaussian (normal) distribution with the appropriate standard deviation according to the table above. \n",
    "To do this:\n",
    "- initialise the random number generator random.seed(10) which ensures that each time you run the code you'll get the same answer.\n",
    "- modifying the code below, generate 6 random noise values scaled appropriately \n",
    "\n",
    " Add the noise to d_true to give simulated measurements (call this \"d\").  Copy and paste the plotting commands from Q3, and add d to your plot as red crosses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the psuedorandom number sequence:\n",
    "random.seed(10)\n",
    "\n",
    "#initialise the noise variable as zeros\n",
    "noise = np.zeros(6)  \n",
    "\n",
    "for i in range(6):\n",
    "    # edit this:\n",
    "    noise[i] = random.normalvariate(0,1)  #generate normally distributed random numbers with mean 0 and standard deviation 1.\n",
    "\n",
    "d = d_true + noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6) Calculate the best linear unbiased estimate. Copy and paste the plotting code from Q5 and add a **smooth** orange line giving your best-fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7) Now repeat the calculations in questions 5 and 6, 10000 times, with a different realisation of the noise each time.\n",
    "Initialise the random number generator as before. One way to achieve this is to create a loop, saving the estimated model parameters for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num = 10000\n",
    "random.seed(10)\n",
    "bootstrapped_models = np.zeros((Num,2))\n",
    "for i in range(Num):\n",
    "    # generate noise vector\n",
    "    # create d\n",
    "    # find BLUE, save in appropriate location of array: bootstrapped_models\n",
    "    # bootstrapped_models[i,:] = ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8) Plot a histogram of each model parameter with an appropriate number of bins, adding a vertical red line indicating the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amend this code:\n",
    "plt.figure()\n",
    "plt.hist( bootstrapped_models[:,0], bins=2);\n",
    "plt.plot( [m_true[0],m_true[0]],[0,1000],'-',color='red') #draw on a red line, with height 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9) Is the estimator (for both model variables) unbiased? Explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10) Plot the 10000 values of the two model parameters against each other as a scatter plot (just use the usual plot command in Python, but plot as circles). Are they correlated? Explain briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11) Calculate the standard deviation of each model parameter from your 10000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np.std(....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13) Is 10000 enough? Gauge how well the standard deviation is converged by plotting the standard deviation from the first n models, against n, where n takes the values 100, 200, 300 etc.\n",
    "Are your values converged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for n in range(100,10100,100):\n",
    "    # plot std of the first n models, against n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12) Calculate the matrix Qmm as defined in lecture 3. Are matrix entries consistent with what you have found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13) Repeat this exercise but use unweighted least squares as the estimator instead of the BLUE. You should be able to copy/paste your code, and only need to modify Qdd. Are the variances better or worse than for the BLUE, and is this what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
